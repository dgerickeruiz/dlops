{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/juansensio/blog/blob/master/090_dlops_onnx/090_dlops_onnx.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DLOps - ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez hemos entrenado varios modelos, los hemos comparado y hemos decidido cu치l usaremos en produccion, tenemos que exportarlo. Para ello tenemos existen alternativas, en funci칩n de la aplicaci칩n (desde desplegar un modelo en dispositivos m칩viles o IoT hasta en servidores en la nube accesibles a trav칠s de una API). En esta serie de posts asumiermos que nuestro modelo ser치 ejecutado en un servidor en la nube, lo cual es lo m치s com칰n ya que de esta manera podemos controlar los recursos computacionales disponibles para su ejecuci칩n, monitorizarlo, desplegar nuevas versiones f치cilmente, etc. En nuestro caso, que hemos entrenado los modelos usando Pytorch y Pytorch Lightning, podr칤amos usar cualquier *framework* en Python que nos permita servir las predicciones a trav칠s de internet, como por ejemplo [Flask](https://flask.palletsprojects.com/en/2.0.x/) o [FastAPI](https://fastapi.tiangolo.com/). El principal problema de esta opci칩n es que tendremos que cargar todas las librer칤as (y sus dependencias) en nuestra API, lo cual resultar치 en una carga muy pesada. Recientemente, Pytorch incluye una soluci칩n dedicada para este caso de uso, [Torchserve](https://pytorch.org/serve/) que si bien nos ofrece una soluci칩n optimizada para servir modelos en producci칩n, est치 limitada al uso de modelos en Pytorch.\n",
    "\n",
    "Es en este punto en el que entra [ONNX](https://onnx.ai/), un est치ndar abierto para la representaci칩n de redes neuronales que permite la interoperabilidad entre librer칤as y ofrece una soluci칩n optimizada para servir modelos en producci칩n (tanto en la nube como on dispositvos m칩viles). De esta manera podemos desacoplar el entrenamiento de los modelos de su puesta en producci칩n, utilizando en cada caso las herramientas preferidas para su desarrollo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exportar un modelo a ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver como podemos exportar un modelo entrenado a ONNX. En primer lugar, cargaremos el *checkpoint* deseado (el cual generamos en el post anterior)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdgerickeruiz\u001b[0m (\u001b[33mdgerickeruiz24\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\INACAP\\2024 Ingenieria\\8vo semestre\\Machine Learning\\Unidad 4\\DLOps\\wandb\\run-20250701_111154-xulmj14e</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dgerickeruiz24/dlops-mnist/runs/xulmj14e' target=\"_blank\">kind-frog-5</a></strong> to <a href='https://wandb.ai/dgerickeruiz24/dlops-mnist' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dgerickeruiz24/dlops-mnist' target=\"_blank\">https://wandb.ai/dgerickeruiz24/dlops-mnist</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dgerickeruiz24/dlops-mnist/runs/xulmj14e' target=\"_blank\">https://wandb.ai/dgerickeruiz24/dlops-mnist/runs/xulmj14e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\INACAP\\2024 Ingenieria\\8vo semestre\\Machine Learning\\Unidad 4\\DLOps\\DLOps\\Lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=784, out_features=100, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=100, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "# 游댐 Inicia una nueva run de wandb\n",
    "wandb.init(project=\"dlops-mnist\", entity=\"dgerickeruiz24\")\n",
    "\n",
    "from src import *\n",
    "\n",
    "module = MNISTModule.load_from_checkpoint('checkpoints/006-val_loss=0.17584-epoch=7.ckpt')\n",
    "module.mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es una buena pr치ctica evaluar nuestro modelo antes y despu칠s de exportarlo para asegurarnos de que todo funciona correctamenete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.949999988079071"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "dm = MNISTDataModule(**module.hparams['datamodule'])\n",
    "dm.setup()\n",
    "\n",
    "def torch_eval():\n",
    "    module.eval()\n",
    "    with torch.no_grad():\n",
    "        preds, labels = torch.tensor([]), torch.tensor([])\n",
    "        for imgs, _labels in dm.val_dataloader():\n",
    "            outputs = module.predict(imgs) > 0.5\n",
    "            preds = torch.cat([preds, outputs.cpu().long()])\n",
    "            labels = torch.cat([labels, _labels])\n",
    "\n",
    "    acc = (preds == labels).float().mean()\n",
    "    return acc.item()\n",
    "\n",
    "torch_eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch Lightning nos permite exportar un modelo a ONNX de manera muy sencilla con la siguiente l칤nea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sample = torch.randint(0, 255, (1, 28, 28), dtype=torch.uint8)\n",
    "module.to_onnx(\n",
    "    'models/binary_classifier_3.onnx', # nombre del modelo a guardar\n",
    "    input_sample, # ejemplo de entrada\n",
    "    export_params=True, # exportar los parametros del modelo\n",
    "    opset_version=11, # en funci칩n de las ops en el modelo, se puede cambiar el opset\n",
    "    input_names = ['input'], # nombre de la entrada\tpara usar en producci칩n\n",
    "    output_names = ['output'],  # nombre de la salida para usar en producci칩n\n",
    "    dynamic_axes={  # para poder tener diferentes batch sizes\n",
    "        'input' : {0 : 'batch_size'},\n",
    "        'output' : {0 : 'batch_size'},\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como ves le tenemos que indicar el nombre del modelo y la carpeta en la que lo queremos guardar, darle unas entradas de ejemplo (que ONNX usar치 para ideantificar todas las operaciones que se realizan dentro del modelo y guardarlas), si queremos exportar los par치metros del modelo, la versi칩n del `opset` (este es el conjunto de operaciones soportadas, que va cambiando a medida que se a침aden nuevas funcionalidades) y, opcionalmente, nombres para entradas y salidas (esto es importante si nuestro modelo tiene varias entradas y/o salidas) as칤 como indicar qu칠 ejes son din치micos (칰til para poder usar el modelo en producci칩n con diferentes *batch sizes*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ONNXRuntime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez hemos exportado nuestro modelo podemos cargarlo y ejecutarlo usando el *ONNXRuntime*. Esta es una de las ventajas de ONNX, y es que existen *runtimes* para multiples entornos y lenguajes. As칤 pues, podr치s entrenar el modelo en `Python` y luego desplegarlo tanto en `Python` como en la web con `Javascript`, en dipositivos moviles con `Android` o `iOS`, etc.\n",
    "\n",
    "> En Python, puedes instalarlo con el comando `pip install onnxruntime`.\n",
    "\n",
    "Para cargar el modelo instanciamos una `InferenceSession` con el `path` al modelo exportado. Luego, definiermos las entradas al modelo usando el un `dict` con el nombre definido en la fase de exportaci칩n. Date cuenta que ahora el modelo usar치 como entradas array de `NumPy`, ya que en este entorno `Pytorch` ya no existe. Si que es importante, sin embargo, que uses el mismo tama침o y tipo de datos que usaste en el entrenamiento. Por 칰ltimo, podemos obtener las salidas del modelo usando el m칠todo `run()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import onnxruntime as ort \n",
    "import numpy as np\n",
    "\n",
    "ort_session = ort.InferenceSession('models/binary_classifier_3.onnx')\n",
    "\n",
    "ort_inputs = {\n",
    "    \"input\": np.random.randint(0, 255, (10, 28, 28), dtype=np.uint8),\n",
    "}\n",
    "\n",
    "ort_output = ort_session.run(['output'], ort_inputs)\n",
    "ort_output[0].shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y, como comentaba antes, es importante evaluar el modelo para asegurarnos que lo hemos exportado bien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "def onnx_eval():\n",
    "    with torch.no_grad():\n",
    "        preds, labels = [], torch.tensor([])\n",
    "        for imgs, _labels in dm.val_dataloader():\n",
    "            ort_inputs = {\n",
    "                \"input\": imgs.numpy(),\n",
    "            }\n",
    "            ort_output = ort_session.run(['output'], ort_inputs)[0]\n",
    "            outputs = sigmoid(ort_output) > 0.5\n",
    "            preds += outputs.astype(int).tolist()\n",
    "            labels = torch.cat([labels, _labels])\n",
    "    acc = (np.array(preds) == labels.numpy()).astype(float).mean()\n",
    "    return acc \n",
    "\n",
    "onnx_eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso obtenemos la misma m칠trica de evaluaci칩n que la obtenida con el `checkpoint`, as칤 que podemos estar tranquilos de que el modelo se comportar치 bien en producci칩n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Versionando modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez hemos exportado nuestro modelo y hemos verificado que funciona bien podemos versionarlo de la misma manera que hicimos con nuestro dataset.\n",
    "\n",
    "Para ello, primero a침adimos la carpeta `models` a nuestro repositorio.\n",
    "\n",
    "```\n",
    "dvc add models\n",
    "```\n",
    "\n",
    "De momento solo tenemos un modelo, el que acabamos de exportar, y podemos sincornizarlo con el repositorio remoto (en el que ya viven varias versiones de nuestro dataset) de la siguiente manera\n",
    "\n",
    "```\n",
    "dvc push\n",
    "```\n",
    "\n",
    "De esta manera, cualquier persona con accesso al proyecto en el que estamos trabajando podr치 recuperar este modelo con el comando\n",
    "\n",
    "```\n",
    "dvc pull models.dvc\n",
    "```\n",
    "\n",
    "Puedes porbar que funcione eliminando la carpeta `models` y recuper치ndola con el comando anterior. Recuerda generar un nuevo tag y sincronizar con el repositorio de git.\n",
    "\n",
    "```\n",
    "git add .\n",
    "git commit -m \"primer modelo\"\n",
    "git push\n",
    "git tag -a v3 -m \"version 3\"\n",
    "git push origin --tags\n",
    "```\n",
    "\n",
    "A partir de ahora, al entrenar nuevos modelos, podemos a침adrilos al repositorio con nuevo tag. Usar un modelo diferente en producci칩n ser치 tan sencillo como cambiar al tag adecuado, lo cual veremos m치s adelante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este post hemos visto como podemos exportar nuestros modelos entrenados con `Pytorch Lightning`a `ONNX`. Esta representaci칩n intermedia nos permitir치 desplegar nuestros modelos en entornos de producci칩n gracias a la `ONNXRuntime`, la cual nos permite ejectuar el modelo de manera optimizada en multitud de entornos (Python, Web, Android, etc.). Adem치s, hemos visto como trackear diferentes versiones de nuestro modelo con `DVC` de la misma manera que lo hacemos con nuestros datos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLOps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
